{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tweepy\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credentials \n",
    "consumer_key=''\n",
    "consumer_secret=''\n",
    "key=''\n",
    "secret=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape tweets using Twitter API\n",
    "#to show example I have used fetched tweets for only 1 screen name @realself\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(key, secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "a=api.user_timeline(screen_name='realself',count=100)\n",
    "\n",
    "\n",
    "\n",
    "a=api.user_timeline(screen_name='realself',count=100)\n",
    "realself_tweets=[]\n",
    "realself_tweets.extend(a)\n",
    "\n",
    "file = open('tweet.json', 'w', encoding='utf8') \n",
    "for status in realself_tweets:\n",
    "        json.dump(status._json,file,sort_keys = True,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets cleaner \n",
    "tok = WordPunctTokenizer()\n",
    "#Patterns to remove links : @ , https, www\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "#change the negative words\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "#Using BeautifulSoup to parse through the tweets text\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "#Remove the unnessesary white space \n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../version1tweets.csv\",header=None,usecols=[0,5],names=['sentiment','text'],encoding='latin-1')\n",
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the tweets...\n",
      "\n",
      "Tweets 100000 of 1600000 has been processed\n",
      "Tweets 200000 of 1600000 has been processed\n",
      "Tweets 300000 of 1600000 has been processed\n",
      "Tweets 400000 of 1600000 has been processed\n",
      "Tweets 500000 of 1600000 has been processed\n",
      "Tweets 600000 of 1600000 has been processed\n",
      "Tweets 700000 of 1600000 has been processed\n",
      "Tweets 800000 of 1600000 has been processed\n",
      "Tweets 900000 of 1600000 has been processed\n",
      "Tweets 1000000 of 1600000 has been processed\n",
      "Tweets 1100000 of 1600000 has been processed\n",
      "Tweets 1200000 of 1600000 has been processed\n",
      "Tweets 1300000 of 1600000 has been processed\n",
      "Tweets 1400000 of 1600000 has been processed\n",
      "Tweets 1500000 of 1600000 has been processed\n",
      "Tweets 1600000 of 1600000 has been processed\n",
      "CPU times: user 8min 21s, sys: 25.9 s, total: 8min 47s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (\"Cleaning the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(0,len(df)):\n",
    "    if( (i+1)%100000 == 0 ):\n",
    "        print (\"Tweets %d of %d has been processed\" % ( i+1, len(df) ))                                                                    \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(df['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the processed twets\n",
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query : To get all the tweets with hastag #Seatle and show counts of unique hastags in the fetched tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['babies',\n",
       "  'BabyYodaProblems',\n",
       "  'BabiesLivesMatter',\n",
       "  'cutebaby',\n",
       "  'CutenessOverload'],\n",
       " ['security', 'Seattle'],\n",
       " [],\n",
       " ['Seattle'],\n",
       " ['seattle', 'tacomawashington']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch 100 the #Seattle\n",
    "b=api.search(q='#Seattle',count=100)\n",
    "#Using regular expression filter out all the hastags\n",
    "all_tweets = [re.findall(r\"#(\\w+)\", tweet.text) for tweet in b ]\n",
    "all_tweets[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all the hastags into lowercase\n",
    "merged = list(itertools.chain.from_iterable(all_tweets))\n",
    "merged1 =map(str.lower, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seattle                     81\n",
       "tacomawashington            10\n",
       "corvid                       5\n",
       "zerostatereflex              5\n",
       "birds                        5\n",
       "wabirds                      5\n",
       "crow                         5\n",
       "seattlestrong                2\n",
       "seat                         2\n",
       "tiroteo                      2\n",
       "yalit                        2\n",
       "washington                   2\n",
       "cute                         2\n",
       "bekindtooneanother           2\n",
       "georgroth                    1\n",
       "babieslivesmatter            1\n",
       "cake                         1\n",
       "whenitrainsitpours           1\n",
       "babies                       1\n",
       "jazzal                       1\n",
       "madeinusa                    1\n",
       "wa                           1\n",
       "bradleytusk                  1\n",
       "thursdayvibes                1\n",
       "art                          1\n",
       "onevoice1                    1\n",
       "youthambassadors             1\n",
       "guncontrolnow                1\n",
       "hotels                       1\n",
       "church                       1\n",
       "                            ..\n",
       "viru                         1\n",
       "seattleshooting              1\n",
       "babyyodaproblems             1\n",
       "supplychain                  1\n",
       "threepigsandwolf             1\n",
       "banking                      1\n",
       "tourism                      1\n",
       "local                        1\n",
       "cakedesign                   1\n",
       "cleancar                     1\n",
       "threepigscake                1\n",
       "enoughisenough               1\n",
       "detailxpertsofpugetsound     1\n",
       "rainydays                    1\n",
       "autodetailing                1\n",
       "icymi                        1\n",
       "curbio                       1\n",
       "northwest                    1\n",
       "mickmulvaney                 1\n",
       "coronovirus                  1\n",
       "blockchain                   1\n",
       "chicago                      1\n",
       "boston                       1\n",
       "story                        1\n",
       "startups                     1\n",
       "valet                        1\n",
       "cutebaby                     1\n",
       "coronavirus                  1\n",
       "seattlestudio                1\n",
       "wuhan                        1\n",
       "Length: 78, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the value count of all the distinct hastags\n",
    "merged = pd.Series(merged1)\n",
    "merged.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
